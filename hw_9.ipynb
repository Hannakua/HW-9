{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrfd6FzSLrE2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras import metrics\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "E5L3LUIi8PDV",
    "outputId": "0a457349-c6af-4076-d5df-fed49405e616"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_data = train_data.reshape(60000, 784)\n",
    "test_data = test_data.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rscaAuBNJdq9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "       0.00392157, 0.        , 0.01176471, 0.00392157, 0.        ,\n",
       "       0.00392157, 0.        , 0.00392157, 0.        , 0.00392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.15686275, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.03921569, 0.37647059, 0.68235294, 0.50196078, 0.49411765,\n",
       "       0.44705882, 0.32941176, 0.39215686, 0.44705882, 0.30196078,\n",
       "       0.38431373, 0.30196078, 0.30196078, 0.36470588, 0.3372549 ,\n",
       "       0.32941176, 0.34117647, 0.41960784, 0.35686275, 0.36862745,\n",
       "       0.34117647, 0.34117647, 0.3372549 , 0.35686275, 0.37647059,\n",
       "       0.40392157, 0.48235294, 0.10196078, 0.34117647, 0.56470588,\n",
       "       0.16470588, 0.40392157, 0.68627451, 0.65490196, 0.25882353,\n",
       "       0.45882353, 0.43921569, 0.52156863, 0.44705882, 0.4745098 ,\n",
       "       0.54901961, 0.51372549, 0.43137255, 0.58431373, 0.51372549,\n",
       "       0.43137255, 0.5372549 , 0.50196078, 0.43921569, 0.41176471,\n",
       "       0.65490196, 0.72941176, 0.54117647, 0.38431373, 0.52941176,\n",
       "       0.24705882, 0.23921569, 0.54901961, 0.36862745, 0.61176471,\n",
       "       0.62745098, 0.56862745, 0.50196078, 0.25490196, 0.35686275,\n",
       "       0.80392157, 0.52156863, 0.64705882, 0.63921569, 0.29411765,\n",
       "       0.38431373, 0.62745098, 0.65882353, 0.54901961, 0.70980392,\n",
       "       0.48627451, 0.36470588, 0.51372549, 0.72156863, 0.70980392,\n",
       "       0.48627451, 0.46666667, 0.76862745, 0.14901961, 0.39215686,\n",
       "       0.65882353, 0.66666667, 0.49411765, 0.63137255, 0.69411765,\n",
       "       0.74901961, 0.24705882, 0.45490196, 0.66666667, 0.6       ,\n",
       "       0.60392157, 0.48235294, 0.29411765, 0.35686275, 0.51372549,\n",
       "       0.6745098 , 0.65882353, 0.74901961, 0.48627451, 0.2745098 ,\n",
       "       0.66666667, 0.63137255, 0.45882353, 0.6       , 0.80392157,\n",
       "       0.85882353, 0.18431373, 0.32156863, 0.74117647, 0.64705882,\n",
       "       0.63137255, 0.63921569, 0.77647059, 0.68627451, 0.38431373,\n",
       "       0.76862745, 0.56470588, 0.6745098 , 0.49411765, 0.74117647,\n",
       "       0.61176471, 0.43921569, 0.72156863, 0.49411765, 0.74117647,\n",
       "       0.68627451, 0.68235294, 0.4745098 , 0.6745098 , 0.75686275,\n",
       "       0.71372549, 0.74901961, 0.74117647, 0.81176471, 0.20392157,\n",
       "       0.36470588, 0.61176471, 0.37647059, 0.65882353, 0.69411765,\n",
       "       0.61960784, 0.56470588, 0.4745098 , 0.32941176, 0.58431373,\n",
       "       0.75686275, 0.75686275, 0.52156863, 0.28627451, 0.39215686,\n",
       "       0.43137255, 0.61176471, 0.90196078, 0.55686275, 0.2745098 ,\n",
       "       0.4       , 0.49411765, 0.71372549, 0.78431373, 0.78431373,\n",
       "       0.61960784, 0.58431373, 0.11372549, 0.45490196, 0.54117647,\n",
       "       0.56470588, 0.71372549, 0.69411765, 0.6745098 , 0.70980392,\n",
       "       0.74901961, 0.21176471, 0.56862745, 0.90980392, 0.35686275,\n",
       "       0.32156863, 0.64705882, 0.75686275, 0.37647059, 0.36470588,\n",
       "       0.86666667, 0.40392157, 0.30980392, 0.72941176, 0.79215686,\n",
       "       0.64705882, 0.74117647, 0.84705882, 0.65490196, 0.45490196,\n",
       "       0.16470588, 0.25490196, 0.56862745, 0.42745098, 0.46666667,\n",
       "       0.71372549, 0.77647059, 0.34117647, 0.43137255, 0.6745098 ,\n",
       "       0.64705882, 0.36470588, 0.26666667, 0.70196078, 0.77647059,\n",
       "       0.75686275, 0.6       , 0.29411765, 0.43921569, 0.65490196,\n",
       "       0.74901961, 0.56470588, 0.41176471, 0.70980392, 0.85882353,\n",
       "       0.68627451, 0.6745098 , 0.63921569, 0.23137255, 0.21176471,\n",
       "       0.85490196, 0.68627451, 0.3372549 , 0.54117647, 0.61176471,\n",
       "       0.22745098, 0.69411765, 0.74901961, 0.86666667, 0.43137255,\n",
       "       0.22745098, 0.3372549 , 0.65882353, 0.66666667, 0.40392157,\n",
       "       0.42745098, 0.46666667, 0.72156863, 0.74117647, 0.63921569,\n",
       "       0.21960784, 0.48235294, 0.77647059, 0.4       , 0.77647059,\n",
       "       0.81176471, 0.23921569, 0.36470588, 0.65882353, 0.64705882,\n",
       "       0.50196078, 0.30196078, 0.32156863, 0.45882353, 0.51372549,\n",
       "       0.45882353, 0.61960784, 0.31372549, 0.58431373, 0.7372549 ,\n",
       "       0.43921569, 0.48235294, 0.63921569, 0.61960784, 0.43137255,\n",
       "       0.68627451, 0.63921569, 0.6       , 0.51372549, 0.36470588,\n",
       "       0.3372549 , 0.48627451, 0.79215686, 0.80392157, 0.34901961,\n",
       "       0.21960784, 0.69411765, 0.6       , 0.34901961, 0.66666667,\n",
       "       0.50196078, 0.34117647, 0.82745098, 0.60392157, 0.71372549,\n",
       "       0.60392157, 0.68627451, 0.85490196, 0.48627451, 0.45490196,\n",
       "       0.99215686, 0.68627451, 0.52156863, 0.8       , 0.74901961,\n",
       "       0.84705882, 0.54117647, 0.39215686, 0.68235294, 0.46666667,\n",
       "       0.59215686, 0.90196078, 0.2       , 0.23921569, 0.45490196,\n",
       "       0.48235294, 0.60392157, 0.86666667, 0.56470588, 0.30980392,\n",
       "       0.69411765, 0.75686275, 0.75686275, 0.6       , 0.42745098,\n",
       "       0.7372549 , 0.56862745, 0.54117647, 0.81960784, 0.4745098 ,\n",
       "       0.50980392, 0.83137255, 0.82745098, 0.85882353, 0.42745098,\n",
       "       0.5372549 , 1.        , 0.76470588, 0.44705882, 0.6       ,\n",
       "       0.28235294, 0.3372549 , 0.4       , 0.44705882, 0.54117647,\n",
       "       0.69411765, 0.50980392, 0.6745098 , 0.78431373, 0.70980392,\n",
       "       0.6745098 , 0.83137255, 0.58431373, 0.52156863, 0.37647059,\n",
       "       0.41176471, 0.39215686, 0.59215686, 0.66666667, 0.63921569,\n",
       "       0.72156863, 0.84705882, 0.68627451, 0.4       , 0.8       ,\n",
       "       0.82745098, 0.45490196, 0.48627451, 0.28627451, 0.28235294,\n",
       "       0.44705882, 0.54117647, 0.3372549 , 0.43921569, 0.57647059,\n",
       "       0.57647059, 0.54901961, 0.6745098 , 0.64705882, 0.57647059,\n",
       "       0.6       , 0.36862745, 0.43921569, 0.36862745, 0.45882353,\n",
       "       0.61176471, 0.59215686, 0.71372549, 0.64705882, 0.63137255,\n",
       "       0.69411765, 0.63137255, 0.64705882, 0.35686275, 0.50980392,\n",
       "       0.58431373, 0.19215686, 0.2745098 , 0.4745098 , 0.76470588,\n",
       "       0.68627451, 0.69411765, 0.75686275, 0.43921569, 0.31372549,\n",
       "       0.77647059, 0.60392157, 0.4       , 0.42745098, 0.76470588,\n",
       "       0.74117647, 0.66666667, 0.70980392, 0.43137255, 0.22745098,\n",
       "       0.69411765, 0.81960784, 0.41960784, 0.23921569, 0.70980392,\n",
       "       0.81176471, 0.70196078, 0.81176471, 0.66666667, 0.24705882,\n",
       "       0.18431373, 0.43921569, 0.60392157, 0.63137255, 0.69411765,\n",
       "       0.46666667, 0.19215686, 0.38431373, 0.55686275, 0.41960784,\n",
       "       0.43921569, 0.46666667, 0.63921569, 0.72941176, 0.70980392,\n",
       "       0.7372549 , 0.51372549, 0.4       , 0.52156863, 0.61960784,\n",
       "       0.51372549, 0.36862745, 0.57647059, 0.72156863, 0.75686275,\n",
       "       0.80392157, 0.61960784, 0.16862745, 0.3372549 , 0.78431373,\n",
       "       0.68235294, 0.63137255, 0.61176471, 0.81960784, 0.60392157,\n",
       "       0.42745098, 0.68627451, 0.66666667, 0.39215686, 0.63137255,\n",
       "       0.71372549, 0.68235294, 0.68235294, 0.77647059, 0.65490196,\n",
       "       0.43921569, 0.69411765, 0.64705882, 0.43921569, 0.55686275,\n",
       "       0.75686275, 0.70196078, 0.76470588, 0.79215686, 0.76470588,\n",
       "       0.39215686, 0.28627451, 0.50980392, 0.54117647, 0.71372549,\n",
       "       0.68627451, 0.60392157, 0.57647059, 0.46666667, 0.7372549 ,\n",
       "       0.72156863, 0.59215686, 0.56862745, 0.6745098 , 0.6745098 ,\n",
       "       0.62745098, 0.51372549, 0.52941176, 0.5372549 , 0.69411765,\n",
       "       0.7372549 , 0.56470588, 0.57647059, 0.63921569, 0.70980392,\n",
       "       0.7372549 , 0.63921569, 0.51372549, 0.23921569, 0.34117647,\n",
       "       0.42745098, 0.44705882, 0.80392157, 0.61176471, 0.39215686,\n",
       "       0.31372549, 0.54901961, 0.68235294, 0.72156863, 0.65882353,\n",
       "       0.34117647, 0.54117647, 0.65882353, 0.61960784, 0.41176471,\n",
       "       0.35686275, 0.4745098 , 0.66666667, 0.71372549, 0.50196078,\n",
       "       0.54901961, 0.48627451, 0.42745098, 0.70980392, 0.37647059,\n",
       "       0.4       , 0.36470588, 0.17647059, 0.38431373, 0.36862745,\n",
       "       0.4745098 , 0.48627451, 0.55686275, 0.45882353, 0.45490196,\n",
       "       0.36862745, 0.66666667, 0.57647059, 0.43137255, 0.48627451,\n",
       "       0.48235294, 0.41176471, 0.43137255, 0.39215686, 0.36862745,\n",
       "       0.37647059, 0.46666667, 0.45490196, 0.43921569, 0.54901961,\n",
       "       0.4       , 0.54901961, 0.48235294, 0.61176471, 0.2745098 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data / 255.0 \n",
    "test_data = test_data / 255.0 \n",
    "\n",
    "train_data[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = vectorize_sequences(train_labels)\n",
    "test_labels = vectorize_sequences(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9zUfd1aeKH3e",
    "outputId": "19690c10-b9d7-4911-b826-260515752e0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[25:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "kYVSvpNIMSnG",
    "outputId": "38f5442b-e2a2-4e9d-db6a-af5883d208a1"
   },
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(750, input_dim=train_data.shape[1], activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 750)               588750    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                48064     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 637464 (2.43 MB)\n",
      "Trainable params: 637464 (2.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "PTcbSxchKZjd",
    "outputId": "5f793a71-cb1c-4059-a0df-bf1b7a6c1d7c"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", metrics.Recall(), metrics.Precision(), metrics.F1Score(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3573
    },
    "colab_type": "code",
    "id": "H6EIXu1dKoju",
    "outputId": "d824768e-d0d3-4f2f-c2af-5e7d5e2addcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 5s 10ms/step - loss: 0.5809 - accuracy: 0.7977 - recall_9: 0.7289 - precision_9: 0.8610 - f1_score: 0.7958 - val_loss: 0.4023 - val_accuracy: 0.8558 - val_recall_9: 0.8188 - val_precision_9: 0.8896 - val_f1_score: 0.8538\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.4042 - accuracy: 0.8551 - recall_9: 0.8190 - precision_9: 0.8896 - f1_score: 0.8542 - val_loss: 0.3638 - val_accuracy: 0.8650 - val_recall_9: 0.8322 - val_precision_9: 0.8988 - val_f1_score: 0.8632\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.3606 - accuracy: 0.8696 - recall_9: 0.8402 - precision_9: 0.8998 - f1_score: 0.8691 - val_loss: 0.3488 - val_accuracy: 0.8713 - val_recall_9: 0.8473 - val_precision_9: 0.8983 - val_f1_score: 0.8708\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.3351 - accuracy: 0.8778 - recall_9: 0.8518 - precision_9: 0.9042 - f1_score: 0.8773 - val_loss: 0.3266 - val_accuracy: 0.8777 - val_recall_9: 0.8587 - val_precision_9: 0.9024 - val_f1_score: 0.8772\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.3133 - accuracy: 0.8859 - recall_9: 0.8618 - precision_9: 0.9095 - f1_score: 0.8855 - val_loss: 0.3131 - val_accuracy: 0.8857 - val_recall_9: 0.8661 - val_precision_9: 0.9060 - val_f1_score: 0.8843\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2965 - accuracy: 0.8903 - recall_9: 0.8698 - precision_9: 0.9136 - f1_score: 0.8899 - val_loss: 0.3264 - val_accuracy: 0.8807 - val_recall_9: 0.8615 - val_precision_9: 0.9025 - val_f1_score: 0.8772\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2849 - accuracy: 0.8942 - recall_9: 0.8752 - precision_9: 0.9157 - f1_score: 0.8938 - val_loss: 0.3173 - val_accuracy: 0.8868 - val_recall_9: 0.8669 - val_precision_9: 0.9091 - val_f1_score: 0.8861\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2706 - accuracy: 0.8994 - recall_9: 0.8819 - precision_9: 0.9193 - f1_score: 0.8990 - val_loss: 0.3245 - val_accuracy: 0.8817 - val_recall_9: 0.8651 - val_precision_9: 0.9010 - val_f1_score: 0.8796\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2620 - accuracy: 0.9020 - recall_9: 0.8856 - precision_9: 0.9206 - f1_score: 0.9017 - val_loss: 0.3165 - val_accuracy: 0.8842 - val_recall_9: 0.8658 - val_precision_9: 0.9049 - val_f1_score: 0.8827\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2519 - accuracy: 0.9066 - recall_9: 0.8907 - precision_9: 0.9236 - f1_score: 0.9063 - val_loss: 0.3074 - val_accuracy: 0.8888 - val_recall_9: 0.8751 - val_precision_9: 0.9054 - val_f1_score: 0.8887\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2394 - accuracy: 0.9097 - recall_9: 0.8950 - precision_9: 0.9264 - f1_score: 0.9094 - val_loss: 0.3041 - val_accuracy: 0.8907 - val_recall_9: 0.8771 - val_precision_9: 0.9079 - val_f1_score: 0.8901\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2355 - accuracy: 0.9125 - recall_9: 0.8981 - precision_9: 0.9279 - f1_score: 0.9123 - val_loss: 0.3104 - val_accuracy: 0.8953 - val_recall_9: 0.8826 - val_precision_9: 0.9065 - val_f1_score: 0.8948\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2283 - accuracy: 0.9154 - recall_9: 0.9012 - precision_9: 0.9288 - f1_score: 0.9151 - val_loss: 0.3068 - val_accuracy: 0.8889 - val_recall_9: 0.8777 - val_precision_9: 0.9023 - val_f1_score: 0.8896\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.2208 - accuracy: 0.9166 - recall_9: 0.9036 - precision_9: 0.9311 - f1_score: 0.9163 - val_loss: 0.3370 - val_accuracy: 0.8863 - val_recall_9: 0.8742 - val_precision_9: 0.8994 - val_f1_score: 0.8855\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2145 - accuracy: 0.9186 - recall_9: 0.9067 - precision_9: 0.9311 - f1_score: 0.9183 - val_loss: 0.3074 - val_accuracy: 0.8936 - val_recall_9: 0.8806 - val_precision_9: 0.9077 - val_f1_score: 0.8920\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.2063 - accuracy: 0.9227 - recall_9: 0.9108 - precision_9: 0.9355 - f1_score: 0.9225 - val_loss: 0.3100 - val_accuracy: 0.8940 - val_recall_9: 0.8846 - val_precision_9: 0.9050 - val_f1_score: 0.8944\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2016 - accuracy: 0.9246 - recall_9: 0.9135 - precision_9: 0.9361 - f1_score: 0.9244 - val_loss: 0.3387 - val_accuracy: 0.8842 - val_recall_9: 0.8750 - val_precision_9: 0.8974 - val_f1_score: 0.8790\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1931 - accuracy: 0.9262 - recall_9: 0.9156 - precision_9: 0.9372 - f1_score: 0.9260 - val_loss: 0.3264 - val_accuracy: 0.8939 - val_recall_9: 0.8858 - val_precision_9: 0.9034 - val_f1_score: 0.8929\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1855 - accuracy: 0.9292 - recall_9: 0.9202 - precision_9: 0.9391 - f1_score: 0.9289 - val_loss: 0.3106 - val_accuracy: 0.8956 - val_recall_9: 0.8866 - val_precision_9: 0.9070 - val_f1_score: 0.8953\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1801 - accuracy: 0.9321 - recall_9: 0.9227 - precision_9: 0.9412 - f1_score: 0.9319 - val_loss: 0.3682 - val_accuracy: 0.8918 - val_recall_9: 0.8831 - val_precision_9: 0.9031 - val_f1_score: 0.8906\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1791 - accuracy: 0.9318 - recall_9: 0.9229 - precision_9: 0.9414 - f1_score: 0.9317 - val_loss: 0.3117 - val_accuracy: 0.8978 - val_recall_9: 0.8882 - val_precision_9: 0.9091 - val_f1_score: 0.8976\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1709 - accuracy: 0.9336 - recall_9: 0.9255 - precision_9: 0.9424 - f1_score: 0.9334 - val_loss: 0.3298 - val_accuracy: 0.8903 - val_recall_9: 0.8817 - val_precision_9: 0.9020 - val_f1_score: 0.8886\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1666 - accuracy: 0.9379 - recall_9: 0.9298 - precision_9: 0.9457 - f1_score: 0.9378 - val_loss: 0.3262 - val_accuracy: 0.8967 - val_recall_9: 0.8904 - val_precision_9: 0.9067 - val_f1_score: 0.8972\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1673 - accuracy: 0.9364 - recall_9: 0.9282 - precision_9: 0.9440 - f1_score: 0.9362 - val_loss: 0.3397 - val_accuracy: 0.8963 - val_recall_9: 0.8906 - val_precision_9: 0.9050 - val_f1_score: 0.8959\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1600 - accuracy: 0.9387 - recall_9: 0.9314 - precision_9: 0.9460 - f1_score: 0.9386 - val_loss: 0.3531 - val_accuracy: 0.8919 - val_recall_9: 0.8844 - val_precision_9: 0.9013 - val_f1_score: 0.8932\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1516 - accuracy: 0.9420 - recall_9: 0.9359 - precision_9: 0.9494 - f1_score: 0.9418 - val_loss: 0.3427 - val_accuracy: 0.8968 - val_recall_9: 0.8911 - val_precision_9: 0.9050 - val_f1_score: 0.8955\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1524 - accuracy: 0.9429 - recall_9: 0.9362 - precision_9: 0.9498 - f1_score: 0.9427 - val_loss: 0.3591 - val_accuracy: 0.8930 - val_recall_9: 0.8867 - val_precision_9: 0.9011 - val_f1_score: 0.8926\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1435 - accuracy: 0.9446 - recall_9: 0.9391 - precision_9: 0.9509 - f1_score: 0.9444 - val_loss: 0.3544 - val_accuracy: 0.8949 - val_recall_9: 0.8884 - val_precision_9: 0.9040 - val_f1_score: 0.8955\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1406 - accuracy: 0.9472 - recall_9: 0.9412 - precision_9: 0.9534 - f1_score: 0.9472 - val_loss: 0.3559 - val_accuracy: 0.8969 - val_recall_9: 0.8908 - val_precision_9: 0.9034 - val_f1_score: 0.8957\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1394 - accuracy: 0.9468 - recall_9: 0.9410 - precision_9: 0.9540 - f1_score: 0.9467 - val_loss: 0.3635 - val_accuracy: 0.8983 - val_recall_9: 0.8936 - val_precision_9: 0.9041 - val_f1_score: 0.8984\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_labels, \n",
    "                    batch_size=128, \n",
    "                    epochs=30,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4CX6To0sr2zd",
    "outputId": "3189e07a-6998-4162-ca1e-690014db0cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.9020 - recall_9: 0.8975 - precision_9: 0.9102 - f1_score: 0.9012\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_data[-2000:], test_labels[-2000:], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Була створена нейронна мережа (модель багатошарового персептрону) на основі датасету fashion_mnist для класифікаціїї зображень одягу. Мережа складається з двох прихованих і одного вихідного шару. Вихідний шар складається з 10 нейронів, що дорівнює кількості класів одягу. В якості функції активації на схованих шарах була використана функція активації relu. Для вихідного шару в якості функції активації була використана функція розподілу ймовірност softmax. В якості функції втрат, як для задачі многокласової класифікації, була використана categorical_crossentropy. В якості оптимізатора був викорастан ADAM оптимізатор.\n",
    "Для зменьшення перенавчання системи був використан dropout, більш високі результати були досягнуті за рахунок збільшення кількості нейронів і епох.\n",
    "При розрахунку помітно, що точність на тестових даних меньша, ніж на тренувальних даних. Це відбувається за рахунок того, що модель була навчена на навчальному датасеті train_data. Коли модель працює з зображенями, які вона ніколи не зустрічала (в даному випадку test_data), то її ефективність класифікації трохи знизилася."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fashion_mnist_prevent_overfitting.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
